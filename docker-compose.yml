services:
    dynamicDataCollectorService:
        build: 
            context: ./FractalDynamicDataCollector
            dockerfile: DockerfileDeployNginx
        command: python /srv/flask_app/apis/http/api/endpoints.py
        image: jperdek/dynamic-data-collector2
        ports:
            - "127.0.0.1:5000:5000"
        container_name: dynamicDataCollector
        volumes:
            - source-volume:/EvolutionSPLFramework
            - dataset-products-volume:/dataset
            - evolution-volume:/evolution
        env_file: .env
        environment:
            FLASK_ENV: development
            KNOWLEDGE_EXTRACTOR_SERVER_PORT: ${KNOWLEDGE_EXTRACTOR_SERVER_PORT:-5000}
            KNOWLEDGE_EXTRACTOR_SERVER_ADDRESS: host.docker.internal
            PATH_TO_EVOLUTION_DIRECTORY: ${PATH_TO_EVOLUTION_DIRECTORY:-/evolution}
            PATH_TO_DATASET_DIRECTORY: ${PATH_TO_DATASET_DIRECTORY:-/dataset}
            KNOWLEDGE_BASE_DB_BOLT: ${KNOWLEDGE_BASE_DB_BOLT:-bolt://fractalKnowledgeBase:7688}
            KNOWLEDGE_DB_NAME: ${KNOWLEDGE_DB_NAME:-evolutionKnowledgeBase}
            KNOWLEDGE_BASE_USERNAME: ${KNOWLEDGE_BASE_USERNAME:-neo4j}
            KNOWLEDGE_BASE_PASSWORD: ${KNOWLEDGE_BASE_PASSWORD:-featureNeo4j}
        depends_on:
            fullyAutomatedEvolutionFramework:
                condition: service_started
            volumeCopyInit:
                condition: service_completed_successfully
        networks:
            - buildernet
        healthcheck:
            test: "wget --no-verbose --tries=1 http://localhost:5000 || exit 1"
            interval: 10s
            timeout: 20s
            retries: 5
            start_interval: 30s
    volumeCopyInit:
        build:
            dockerfile: Dockerfile
        command: cp -R /localFiles/splsToMerge /localFiles/resources  ${PROJECT_PATH:-/EvolutionSPLFramework}
        env_file: .env
        image: jperdek/volume-copy-init2
        environment:
            PROJECT_PATH: ${PROJECT_PATH:-/EvolutionSPLFramework}
            PATH_TO_EVOLUTION_DIRECTORY: ${PATH_TO_EVOLUTION_DIRECTORY:-/evolution}
        volumes:
            - dataset-products-volume:/dataset
            - source-volume:/EvolutionSPLFramework
            - evolution-volume:/evolution
    fullyAutomatedEvolutionFramework:
        build:
            context: ./EvolutionSPLFramework
            dockerfile: Dockerfile
        image: jperdek/fully-automated-evolution2
        ports:
            - "127.0.0.1:5002:5002"
        container_name: fully-automated-evolution
        links:
            - fractalKnowledgeBase
            - astConverterAndComplexityAnalysisAPI
        extra_hosts:
            - "host.docker.internal:host-gateway"
        volumes:
            - dataset-products-volume:/dataset
            - source-volume:/EvolutionSPLFramework
            - evolution-volume:/evolution
            - tmp-volume:/temp
        env_file: .env
        environment:
            FLASK_ENV: development
            IS_LINUX: true,
            SHOUD_TERMINATE_IF_KNOWLEDGE_BASE_IS_NOT_INITIALIZED: true,
            KNOWLEDGE_EXTRACTOR_SERVER_PORT: ${KNOWLEDGE_EXTRACTOR_SERVER_PORT:-5000}
            SHARE_FILES_USING_DISK: ${SHARE_FILES_USING_DISK:-true}
            DOCKER_HOST: host.docker.internal
            PROJECT_PATH: ${PROJECT_PATH:-/EvolutionSPLFramework}
            PATH_TO_EVOLUTION_DIRECTORY: ${PATH_TO_EVOLUTION_DIRECTORY:-/evolution}
            CANTO_SCRIPT_RESOURCE_LOCATION: ${CANTO_SCRIPT_RESOURCE_LOCATION:-/EvolutionSPLFramework/resources/canto/canto-0.15.js}
            PATH_TO_TEMP_DIRECTORY: ${PATH_TO_TEMP_DIRECTORY:-/temp}
            QUEUE_EVOLVED_SPL: ${QUEUE_EVOLVED_SPL:-EVOLVED_SPL}
            PROCESSING_QUEUE_HOST: host.docker.internal
            PROCESSING_QUEUE_VHOST: rabbitmq
            MQ_CONSUMER_USER_NAME: ${CONSUMER_USER_NAME:-consumerUser}
            MQ_CONSUMER_USER_PASSWORD: ${CONSUMER_USER_PASSWORD:-consumerUser}
            BUILD_DEFAULT_KNOWLEDGE_BASE: ${BUILD_DEFAULT_KNOWLEDGE_BASE:-true}
            OUTPUT_DEBUG_FILES: ${OUTPUT_DEBUG_FILES:-false}
            OUTPIT_FILES_AS_ANNOTATED_AST_AND_CODE: ${OUTPIT_FILES_AS_ANNOTATED_AST_AND_CODE:-false}
            SHOW_SELECTED_SYNTESIZED_CODE_CONSTRUCTS: ${SHOW_SELECTED_SYNTESIZED_CODE_CONSTRUCTS:-false}
            SHOW_MISSING_EVOLUTION_ENHANCEMENTS: ${SHOW_MISSING_EVOLUTION_ENHANCEMENTS:-false}
            SHOW_POLLUTING_INFORMATION: ${SHOW_POLLUTING_INFORMATION:-false}
            SHOW_CREATED_ENTITIES: ${SHOW_CREATED_ENTITIES:-false}
            SHOW_INFORMATION_ABOUT_EXPORTS: ${SHOW_INFORMATION_ABOUT_EXPORTS:-false}
            SHOW_INITIAL_COPIED_INFORMATION: ${SHOW_INITIAL_COPIED_INFORMATION:-false}
            SHOW_POSITIVE_VARIABILITY_INCREMENT_CODE_FRAGMENT: ${SHOW_POSITIVE_VARIABILITY_INCREMENT_CODE_FRAGMENT:-false}
            PROCESS_STEP_INFORMATION: ${PROCESS_STEP_INFORMATION:-false}
            SHOW_DERIVED_PROJECT_INFORMATION: ${SHOW_DERIVED_PROJECT_INFORMATION:-false}
        networks:
            - buildernet
        depends_on:
            splsToProcessMessageQueue:
                condition: service_started
            astConverterAndComplexityAnalysisAPI:
                condition: service_healthy
            volumeCopyInit:
                condition: service_completed_successfully
            fractalKnowledgeBase:
                condition: service_healthy
    astConverterAndComplexityAnalysisAPI:
        image: jperdek/ast-converter-and-complexity-analysis-api2
        build:
            context: ./ConverterAndComplexityAnalyzerAPI
            dockerfile: Dockerfile
        extra_hosts:
            - "host.docker.internal:host-gateway"
        ports:
            - "127.0.0.1:5001:5001"
        networks:
            - buildernet
        volumes:
            - tmp-volume:/temp
        restart: on-failure
        healthcheck:
            test: "wget --no-verbose --tries=1 http://localhost:5001 || exit 1"
            interval: 5s
            timeout: 15s
            retries: 5
            start_interval: 3s
        environment:
            PATH_TO_TEMP_DIRECTORY: ${PATH_TO_TEMP_DIRECTORY:-/temp}
    fractalKnowledgeBase: 
        build:
            context: ./SemanticBaseNeo4j/fractalKnowledgeBase
            dockerfile: Dockerfile
        image: jperdek/fractal-knowledge-base2
        hostname: neo4j
        ports:
            - "127.0.0.1:7475:7475"
            - "127.0.0.1:7688:7688"
        env_file: .env
        networks:
            - buildernet
        environment:
            NEO4J_AUTH: ${KNOWLEDGE_BASE_USERNAME:-neo4j}/${KNOWLEDGE_BASE_PASSWORD:-featureNeo4j}
            NEO4J_dbms_logs_debug_level: ${KNOWLEDGE_BASE_LOG_LEVEL:-DEBUG}
#            NEO4J_PLUGINS: '["apoc", "n10s"]' # installed in docker compose
        healthcheck:
            test: "wget --no-verbose --tries=1 http://localhost:7475 || exit 1"
            interval: 5s
            timeout: 15s
            retries: 5
            start_interval: 60s
    splCreationConsumer: 
        build:
            context: ./FractalDynamicDataCollector/queueConsumer
            dockerfile: Dockerfile
        image: jperdek/spl-creation-consumer2
        volumes:
            - dataset-products-volume:/dataset
            - source-volume:/EvolutionSPLFramework
            - evolution-volume:/evolution
            - tmp-volume:/temp
        env_file: .env
        environment:
            PROCESSING_QUEUE_USERNAME: ${PROCESSING_QUEUE_USERNAME:-splManager}
            PROCESSING_QUEUE_PASSWORD: ${PROCESSING_QUEUE_PASSWORD:-splManager}
            PROCESSING_QUEUE_VHOST: ${PROCESSING_QUEUE_VHOST:-/evolution}
            CONSUMER_USER_NAME: ${CONSUMER_USER_NAME:-consumerUser}
            CONSUMER_USER_PASSWORD: ${CONSUMER_USER_PASSWORD:-consumerUser}
            RABBIT_MQ_HOST: splsToProcessMessageQueue
            RABBIT_MQ_HEARTBEAT: "0"
            QUEUE_EVOLVED_SPL: ${QUEUE_EVOLVED_SPL:-EVOLVED_SPL}
            KNOWLEDGE_EXTRACTOR_SERVER_ADDRESS: host.docker.internal
            PURGE_QUEUE_ON_START: false
            BUILD_DEFAULT_KNOWLEDGE_BASE: ${BUILD_DEFAULT_KNOWLEDGE_BASE:-true}
        depends_on:
            volumeCopyInit:
                condition: service_completed_successfully
            splsToProcessMessageQueue:
                condition: service_healthy
            dynamicDataCollectorService:
                condition: service_healthy
        restart: on-failure
        networks:
            - buildernet
    splsToProcessMessageQueue:
        image: jperdek/spl-to-process-message-queue2
        build:
            context: ./rabbitMQ
            dockerfile: Dockerfile
            args:
                consumer_user_name: ${CONSUMER_USER_NAME:-consumerUser}
                consumer_user_password: ${CONSUMER_USER_PASSWORD:-consumerUser}
        extra_hosts:
            - "host.docker.internal:host-gateway"
        env_file: .env
        environment:
            RABBITMQ_DEFAULT_USER: ${PROCESSING_QUEUE_USERNAME:-splManager}
            RABBITMQ_DEFAULT_PASS: ${PROCESSING_QUEUE_PASSWORD:-splManager}
            RABBITMQ_DEFAULT_VHOST: ${PROCESSING_QUEUE_VHOST:-rabbitmq}
            CONSUMER_USER_NAME: ${CONSUMER_USER_NAME:-consumerUser}
            CONSUMER_USER_PASSWORD: ${CONSUMER_USER_PASSWORD:-consumerUser}
        ports:
            - "127.0.0.1:15672:15672"
            - "127.0.0.1:5672:5672"
            - "127.0.0.1:5671:5671"
        volumes:
            - rabbitmq-volume:/var/lib/rabbitmq/
        networks:
            - buildernet
        healthcheck:
            test: "wget --no-verbose --tries=1 http://localhost:15672 || exit 1"
            interval: 10s
            timeout: 20s
            retries: 5
            start_interval: 60s
volumes:
    source-volume:
    evolution-volume:
    dataset-products-volume:
    rabbitmq-volume:
    tmp-volume:
networks:
    buildernet:
        driver: "bridge"
        